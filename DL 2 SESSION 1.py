# -*- coding: utf-8 -*-
"""DL 2 SESSION 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14AyC_hSVYl1rXhHPy-rB6l0UuIf7XCj7
"""

import requests
from bs4 import BeautifulSoup
import re
import csv

# URL of the Wikihow page to scrape
url = 'https://www.wikihow.com/Special:Randomizer'

# Send an HTTP request to the URL and receive the HTML conter
response = requests.get(url)
html_content = response.content

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(html_content, 'html.parser') # Fixed the typo here
article_title = soup.find('title').text.strip()
print(article_title)

from google.colab import drive
drive.mount('/content/drive')

article_title = soup.find('title').text.strip()
print(article_title)

# Extract the subheadings and paragraphs using the appropriate HTML tags
subheadings = []
paragraphs = []
steps = soup.find_all('div', {'class': 'step'})
for step in steps:
    subheading_element = step.find('b')
    if(subheading_element is not None):
        subheading_text = subheading_element.text.strip().replace('\n','')
        subheading_text = subheading_text.encode('ascii', errors='ignore').decode()
        subheading_text = re.sub(r", ", "", subheading_text) # Added missing 'string' argument
        print(subheading_text)
        subheadings.append(subheading_text)

        #this block removes titles and extra links
        subheading_element.extract()
        for span_tag in step.find_all('span'):
            span_tag.extract()


        paragraph_text = step.text.strip().replace('\n','').replace('','')
        paragraph_text = paragraph_text.encode('ascii', errors='ignore').decode()
        paragraph_text = re.sub(r'','', paragraph_text)
        print(paragraph_text)
        paragraphs.append(paragraph_text)

if(len(subheadings)):
     with open('/content/wikihow.csv', mode='a', newline='', encoding='utf-8') as csv_file:
         writer = csv.writer(csv_file)
         for i in range(len(subheadings)):
             writer.writerow([article_title, subheadings[i], paragraphs[i]])

for count in range(500):
    # URL of the Wikihow page to scrape
    url = "https://www.wikihow.com/Special:Randomizer"

    # Send an HTTP request to the URL and receive the HTML content
    response = requests.get(url)
    html_content = response.content

     # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')
    article_title = soup.find('title').text.strip()
    print(article_title+" "+str(count))

    # Extract the subheadings and paragraphs using the appropriate HTML tags
    subheadings = [] # Changed - from subheadings - [] to subheadings = []
    paragraphs = []
    steps = soup.find_all('div', {'class': 'step'})
    for step in steps:
        subheading_element = step.find('b') #Fixed indentation
        if(subheading_element is not None):
            subheading_text = subheading_element.text.strip().replace('\n',' ')
            subheading_text = subheading_text.encode('ascii', errors="ignore").decode()
            subheading_text = re.sub(r'', '', subheading_text)
            subheadings.append(subheading_text)
            subheading_element.extract()
            for span_tag in step.find_all('span'):
                span_tag.extract()
            paragraph_text = step.text.strip().replace('\n','')
            paragraph_text = paragraph_text.encode('ascii', errors='ignore' ) .decode()
            paragraph_text = re.sub(r'', "", paragraph_text)
            paragraphs.append(paragraph_text)

        if(len(subheadings)):
            with open('/content/wikihow.csv', mode='a', newline='', encoding="utf-8") as csv_file: # Fixed indentation
                writer = csv.writer(csv_file)
                for i in range(len(subheadings)):
                    writer.writerow([article_title, subheadings[i], paragraphs[i]])